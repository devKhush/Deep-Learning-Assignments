{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/sumdev/MSA_BTP/env/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/mnt/disk1/sumdev/MSA_BTP/env/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Inception_Q3 Architecture on train split of AudioDataset\n",
      "Training Epoch: 0, [Loss: 2.900199957844714, Accuracy: 21.083128776435046]\n",
      "Training Epoch: 1, [Loss: 2.1400794705595496, Accuracy: 40.741833459214504]\n",
      "Training Epoch: 2, [Loss: 1.830655882725903, Accuracy: 49.483100453172206]\n",
      "Training Epoch: 3, [Loss: 1.6320668592193697, Accuracy: 54.76184856495468]\n",
      "Training Epoch: 4, [Loss: 1.4842510992306597, Accuracy: 58.78257175226586]\n",
      "Training Epoch: 5, [Loss: 1.3808395411492835, Accuracy: 61.58775490936556]\n",
      "Validating Inception_Q3 Architecture on val split of AudioDataset\n",
      "Validation Epoch: 0, [Loss: 1.327154873253463, Accuracy: 62.91599025974026]\n",
      "Testing Inception_Q3 Architecture on test split of AudioDataset\n",
      "Testing: [Loss: 1.4227174141827752, Accuracy: 60.818014705882355]\n"
     ]
    }
   ],
   "source": [
    "from Pipeline._2020211A2 import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "imageDataset = [\n",
    "    # ImageDataset(split=\"train\"),\n",
    "    # ImageDataset(split=\"val\"),\n",
    "    # ImageDataset(split=\"test\")    \n",
    "]\n",
    "audioDataset = [\n",
    "    AudioDataset(split=\"train\"),\n",
    "    AudioDataset(split=\"val\"),\n",
    "    AudioDataset(split=\"test\")\n",
    "]\n",
    "   \n",
    "Architectures = [\n",
    "    # Resnet_Q1(),\n",
    "    # VGG_Q2(),\n",
    "    Inception_Q3(),\n",
    "    # CustomNetwork_Q4()\n",
    "]\n",
    "device = 'T'\n",
    "\n",
    "\n",
    "for network in Architectures:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params=network.parameters(),\n",
    "        lr=LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    for dataset in imageDataset + audioDataset:\n",
    "        if dataset.datasplit == \"train\":\n",
    "            print(\n",
    "                \"Training {} Architecture on {} split of {}\".format(\n",
    "                    network.__class__.__name__,\n",
    "                    dataset.datasplit,\n",
    "                    dataset.__class__.__name__\n",
    "                )\n",
    "            )\n",
    "            network.train()\n",
    "            train_dataloader = DataLoader(\n",
    "                dataset=dataset,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                num_workers=2,\n",
    "                drop_last=True\n",
    "            )\n",
    "            \n",
    "            trainer(\n",
    "                gpu=device,\n",
    "                dataloader=train_dataloader,\n",
    "                network=network,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer\n",
    "            )\n",
    "        \n",
    "        elif dataset.datasplit == \"val\":\n",
    "            print(\n",
    "                \"Validating {} Architecture on {} split of {}\".format(\n",
    "                    network.__class__.__name__,\n",
    "                    dataset.datasplit,\n",
    "                    dataset.__class__.__name__\n",
    "                )\n",
    "            )\n",
    "            network.train()\n",
    "            val_dataloader = DataLoader(\n",
    "                dataset=dataset,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                num_workers=2,\n",
    "                drop_last=True\n",
    "            )\n",
    "            \n",
    "            validator(\n",
    "                gpu=device,\n",
    "                dataloader=val_dataloader,\n",
    "                network=network,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer\n",
    "            )\n",
    "            \n",
    "        elif dataset.datasplit == \"test\":\n",
    "            print(\n",
    "                \"Testing {} Architecture on {} split of {}\".format(\n",
    "                    network.__class__.__name__,\n",
    "                    dataset.datasplit,\n",
    "                    dataset.__class__.__name__\n",
    "                )\n",
    "            )\n",
    "            network.eval()\n",
    "            test_dataloader = DataLoader(\n",
    "                dataset=dataset,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                num_workers=2,\n",
    "                drop_last=True\n",
    "            )\n",
    "            evaluator(\n",
    "                dataloader=test_dataloader,\n",
    "                network=network,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
